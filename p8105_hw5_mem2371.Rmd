---
title: "Homework 5"
author: Megan Marziali
output: github_document
---

```{r, message = FALSE}
library(tidyverse)
library(rvest)
library(ggplot2)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 0.6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
```

## Problem 1

Read in the data.

```{r, message = FALSE, warning = FALSE}
homicides_df_orig = 
  read_csv("./data/homicides.csv",
           na = "") %>% 
  janitor::clean_names() %>% 
  mutate(victim_age = as.numeric(victim_age))

homicides_df = 
  homicides_df_orig %>% 
  mutate(
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest" ~ "unsolved",
      disposition == "Closed by arrest" ~ "solved"
      ),
    city_state = str_c(city, state, sep = "_"),
    date = as.Date(paste(reported_date, "01", sep = ""), "%Y%m%d")
  ) %>% 
  select(city_state, resolved) %>% 
  filter(city_state != "Tulsa_AL")
```

The Washington Post data on homicides in 50 large US cities contains `r nrow(homicides_df_orig)` rows and `r ncol(homicides_df_orig)` variables, with `r variable.names(homicides_df_orig)`. The data contains important information regarding homicides, such as the victim's first and last name, race, age, sex, location of incident (including city and state), and the status of the case. The age range of victims is from `r min(pull(homicides_df_orig, var = victim_age), na.rm = TRUE)` years (suggesting missing data), to `r max(pull(homicides_df_orig, var = victim_age), na.rm = TRUE)` years, with a mean of `r mean(pull(homicides_df_orig, var = victim_age), na.rm = TRUE)` (SD = `r sd(pull(homicides_df_orig, var = victim_age), na.rm = TRUE)`) and median of `r median(pull(homicides_df_orig, var = victim_age), na.rm = TRUE)` (IQR: `r IQR(pull(homicides_df_orig, var = victim_age), na.rm = TRUE)`).

The manipulated Washington Post data contains `r nrow(homicides_df)` rows and `r ncol(homicides_df)` variables. This includes `r variable.names(homicides_df)`. 

Let's look at this a bit more.

```{r, message = FALSE, warning = FALSE}
aggregate_df = 
  homicides_df %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )
```

Doing a prop test for a single city.

```{r, message = FALSE, warning = FALSE, echo = FALSE}
prop.test(
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>%  pull(hom_unsolved),
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>%  pull(hom_total)) %>% 
  broom::tidy()
```

Trying to iterate:

```{r, message = FALSE, warning = FALSE}
results_df = 
  aggregate_df %>% 
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)
```


```{r, message = FALSE, warning = FALSE}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

## Problem 2 ideas 

Importing initial dataset.

```{r, message = FALSE, warning = FALSE}
data_1 = read_csv("./lda_data/con_01.csv")
```

Creating code to import all datasets, clean data.

```{r, message = FALSE, warning = FALSE}
path_df = 
  tibble(
    path = list.files("lda_data")
  ) %>% 
  mutate(
    path = str_c("lda_data/", path),
    data = purrr::map(path, read_csv)
    ) %>% 
  unnest(data) %>% 
  separate(
    path,
    into = c("file1", "file2", "trt_arm", "id", NA)
  ) %>% 
  select(-file1, -file2) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "obs"
  ) %>% 
  mutate(
    week = as.numeric(sub(".*_", "", week))
    )
```

Creation of a spaghetti plot.

```{r, message = FALSE, warning = FALSE}
path_df %>% 
  ggplot(aes(x = week, y = obs, group = id, color = id)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = lm, se = FALSE) +
  facet_grid(. ~ trt_arm) +
  labs(y = "Observations", x = "Week", title = "Observations over time for the \ncontrol and experimental treatment arms")
```

## Problem 3

The following code is to build the initial function, and make sure it works when setting mu = 0.

```{r}
sim_ttest2 = function(samp_size = 30, mu, sigma = 5) {
  
  sim_data = 
    tibble(
      x = rnorm(n = samp_size, mean = mu, sd = sigma)
    )
  
  sim_data %>% 
    t.test() %>% 
    broom::tidy()
}

sim_results = 
  rerun(5000, sim_ttest2(mu = 0)) %>% 
  bind_rows()
```

The next code chunk sets mu as 0:6, to create a finalized dataset.

```{r}
sim_tres = 
  tibble(
  mu = c(0, 1, 2, 3, 4, 5, 6)
  ) %>% 
  mutate(
    output_lists = map(.x = mu, ~ rerun(5000, sim_ttest2(mu = .x))),
    estimate_df = map(output_lists, bind_rows)
  ) %>% 
  select(-output_lists) %>% 
  unnest(estimate_df) %>% 
  rename(mu_hat = estimate) %>% 
  select(mu, mu_hat, p.value) %>% 
  mutate(
    rejected = case_when(
      p.value <= 0.05 ~ "yes",
      p.value > 0.05  ~ "no"
      ))
```

#### Plot 1

The plot below illustrates power and the true value of mu.

```{r, message = FALSE, warning = FALSE}
sim_prop = 
  sim_tres %>% 
  group_by(mu, rejected) %>% 
  filter(rejected == "yes") %>% 
  summarise(
    count = n()) %>% 
  mutate(
    prop = count / 5000
  )
  
sim_prop %>% 
  ggplot(aes(x = mu, y = prop)) +
  geom_point() +
  labs(y = "Proportion of times null rejected", x = "True value of mu", title = "Power and true value of mu")
```

From this plot, we can see that as the true value of mu increases, power increases and levels off around 1.0 (which is maximum power).

#### Plot 2

The plot below illustrates the average estimate of mu on the x-axis, and the true value of mu on the y-axis.

```{r, message = FALSE, warning = FALSE}
sim_avg = 
  sim_tres %>%
  group_by(mu) %>%
  summarize(
    mu_avg = mean(mu_hat)
  ) %>% 
  ggplot(aes(x = mu, y = mu_avg)) +
  geom_point() + geom_smooth(method = lm, se = FALSE) +
  labs(y = "True value of mu", x = "Average of sample mu", title = "True value of mu and mu average")
```

The plot below illustrates the average estimate of mu on the x-axis within samples where the null was rejected, and the true value of mu on the y-axis. The two plots have been patched together.

```{r, message = FALSE, warning = FALSE}
sim_avg_rej = 
  sim_tres %>%
  group_by(mu) %>%
  filter(rejected == "yes") %>% 
  summarize(
    mu_avg_rej = mean(mu_hat)) %>% 
  ggplot(aes(x = mu, y = mu_avg_rej)) +
  geom_point() + geom_smooth(method = lm, se = FALSE) +
  labs(y = "True value of mu", x = "Average of sample mu", title = "True value of mu and mu average among samples where null was rejected")

sim_avg / sim_avg_rej
```

As can be viewed from the above plots, the same average of mu is approximately equal to the true value of mu among samples where null is rejected at greater values of mu. At lower values, there is some variation between average sample mu and true value of mu among samples where the null was rejected. This is reasonable, as with higher values of mu, there is a greater likelihood that the power will be sufficient to reject the null.
